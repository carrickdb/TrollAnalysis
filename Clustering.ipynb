{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, datetime, math\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import scale\n",
    "import operator\n",
    "from sklearn.metrics import calinski_harabaz_score\n",
    "from sklearn.cluster import KMeans\n",
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/orig_en_train.json\") as f:\n",
    "    data = json.load(f)\n",
    "    # print(\"number of tweets\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hs = defaultdict(int)\n",
    "hs_likes = defaultdict(int)\n",
    "\n",
    "for d in data:\n",
    "    curr_hashtags = d['hashtags']\n",
    "    curr_hashtags = curr_hashtags[1:-1].split(', ')\n",
    "    curr_hashtags = [x for x in curr_hashtags if x != \"\"]\n",
    "    for hs in curr_hashtags:\n",
    "        if hs!=\"\":\n",
    "            all_hs[hs] += 1\n",
    "    if int(d['like_count']) > 0:\n",
    "        for hs in curr_hashtags:\n",
    "            if hs!=\"\":\n",
    "                hs_likes[hs] += 1\n",
    "\n",
    "sorted_hs = sorted(all_hs.items(), key=operator.itemgetter(1))\n",
    "sorted_hsl = sorted(hs_likes.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "quality_hs = []\n",
    "quality_hsl = []\n",
    "for hs in sorted_hs:\n",
    "    if hs[1]>7:\n",
    "        quality_hs.append(hs)\n",
    "for hs in sorted_hsl:\n",
    "    if hs[1]>7:\n",
    "        quality_hsl.append(hs)\n",
    "\n",
    "hs_index = defaultdict(int)\n",
    "ind_hs = defaultdict(str)\n",
    "for ind, hs in enumerate(quality_hs):\n",
    "    hs_index[hs[0]] = ind\n",
    "    ind_hs[ind] = hs[0]\n",
    "    \n",
    "all_user = set()\n",
    "for d in data:\n",
    "    all_user.add(d['userid'])\n",
    "users = list(all_user)\n",
    "user_ind = {}\n",
    "ind_user = {}\n",
    "user_hs = []\n",
    "for ind, u in enumerate(users):\n",
    "    user_hs.append([0]*len(quality_hs))\n",
    "    user_ind[u] = ind\n",
    "    ind_user[ind] = u\n",
    "    \n",
    "for d in data:\n",
    "    curr_hashtags = d['hashtags']\n",
    "    curr_hashtags = curr_hashtags[1:-1].split(', ')\n",
    "    curr_hashtags = [x for x in curr_hashtags if x != \"\"]\n",
    "    for hs in curr_hashtags:\n",
    "        user_hs[user_ind[d['userid']]][hs_index[hs]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_user_hs = []\n",
    "for d in user_hs:\n",
    "    if d != [0]*len(quality_hs):\n",
    "        filtered_user_hs.append(d)\n",
    "filtered_user_hs = np.array(filtered_user_hs, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_hs_sums = []\n",
    "for d in filtered_user_hs:\n",
    "    user_hs_sums.append(sum(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(filtered_user_hs.shape[0]):\n",
    "    filtered_user_hs[i] = np.divide(filtered_user_hs[i], user_hs_sums[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/santi/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:164: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/home/santi/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/data.py:181: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": [
    "scaled_user_hs = scale(filtered_user_hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 26.15858515390863\n",
      "3 19.136288237237693\n",
      "4 19.36554009525447\n",
      "5 10.529072244753463\n",
      "6 14.879765551453158\n",
      "7 12.57472559291928\n",
      "8 12.285952328690142\n",
      "9 12.725429928950879\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=0).fit(scaled_user_hs)\n",
    "    labels = kmeans.labels_\n",
    "    centers = kmeans.cluster_centers_\n",
    "    print(i, calinski_harabaz_score(scaled_user_hs, labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0).fit(scaled_user_hs)\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "sorted_center_hs = []\n",
    "for d in centers:\n",
    "    sorted_center_hs.append(np.argsort(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics\n",
      "WELOVEYOUTOONIALL\n",
      "ccot\n",
      "phosphorusdisaster\n",
      "WakeUpAmerica\n",
      "TrumpForPresident\n",
      "iTunes\n",
      "StopTheGOP\n",
      "quotes\n",
      "PJNET\n",
      "GOPDebate\n",
      "DeadHorse\n",
      "KochFarms\n",
      "IslamKills\n",
      "tcot\n",
      "BlackLivesMatter\n",
      "Chernobyl2015\n",
      "ColumbianChemicals\n",
      "FukushimaAgain\n",
      "ThingsMoreTrustedThanHillary\n"
     ]
    }
   ],
   "source": [
    "for d in sorted_center_hs[0][-20:]:\n",
    "    print(ind_hs[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
