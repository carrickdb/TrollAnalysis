{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, datetime, math\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import datetime\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import scale\n",
    "import operator\n",
    "from sklearn.metrics import calinski_harabaz_score\n",
    "from sklearn.cluster import KMeans\n",
    "import wordcloud\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_train_labels = []\n",
    "tweet_test_labels = []\n",
    "with open(\"cluster_labels\", \"rb\") as f:\n",
    "    tweet_train_labels = pickle.load(f)\n",
    "    tweet_test_labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_train_labels = [[x] for x in tweet_train_labels]\n",
    "tweet_test_labels = [[x] for x in tweet_test_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_train_labels = np.array(tweet_train_labels)\n",
    "tweet_test_labels = np.array(tweet_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe = OneHotEncoder()\n",
    "ohe.fit(tweet_train_labels)\n",
    "ohe_tweet_train_cluster_labels = ohe.transform(tweet_train_labels).toarray()\n",
    "ohe_tweet_test_cluster_labels = ohe.transform(tweet_test_labels).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ohe_tweet_test_cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    }
   ],
   "source": [
    "import json, statistics, pprint, re\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "with open(\"data/orig_en_train.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "split = len(data)//2\n",
    "train = data[split:]\n",
    "valid = data[:split]\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "account_likes = {}\n",
    "tweets = {}\n",
    "tweet_clients = defaultdict(int)\n",
    "for d in train:\n",
    "    tweet_clients[d['tweet_client_name']] += 1\n",
    "    # tweet_text = d['tweet_text']\n",
    "    # if tweet_text in tweets:\n",
    "    #     tweets[tweet_text].append(int(d['like_count']))\n",
    "    # else:\n",
    "    #     tweets[tweet_text] = [int(d['like_count'])]\n",
    "    user = d['userid']\n",
    "    if user in account_likes:\n",
    "        account_likes[user].append(int(d['like_count']))\n",
    "    else:\n",
    "        account_likes[user] = [int(d['like_count'])]\n",
    "#     if int(d['like_count']) > 10:\n",
    "#         pprint(d)\n",
    "#         assert False\n",
    "\n",
    "\n",
    "clients = {}\n",
    "i = 0\n",
    "for client, count in tweet_clients.items():\n",
    "    clients[client] = i\n",
    "    i += 1\n",
    "\n",
    "for user, likes in account_likes.items():\n",
    "    account_likes[user] = statistics.median(likes)\n",
    "\n",
    "popular_hashtags = {'sports': 0, 'politics': 1, 'local': 2, 'world': 3, 'news': 4, 'SanDiego': 5, 'Syria': 6, \\\n",
    "                    'ISIS': 7, 'WakeUpAmerica': 8, 'Trump': 9, 'business': 10, 'Chicago': 11, 'health': 12, \\\n",
    "                    'love': 13, 'TopNews': 14, 'BLM': 15, 'PoliceBrutality': 16, 'Obama': 17, 'BlackTwitter': 18, \\\n",
    "                    'Trump2016': 19}\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    if len(re.findall(\"(https://[^\\s]+)\", tweet)) > 0:\n",
    "        tweet = re.sub(\"(https://[^\\s]+)\", \"\", tweet)\n",
    "    if len(re.findall(\"(http://[^\\s]+)\", tweet)) > 0:\n",
    "        tweet = re.sub(\"(http://[^\\s]+)\", \"\", tweet)\n",
    "    if len(re.findall('@[^\\s@]+', tweet)) > 0:\n",
    "        tweet = re.sub('@[^\\s@]+', \"\", tweet)\n",
    "    if len(re.findall(\"(#[^#\\s]+)\", tweet)) > 0:\n",
    "        tweet = re.sub(\"(#[^#\\s]+)\", \"\", tweet)\n",
    "    return tweet\n",
    "\n",
    "corpus = []\n",
    "for d in train:\n",
    "    corpus.append(process_tweet(d['tweet_text']))\n",
    "    # corpus.append(d['tweet_text'])\n",
    "\n",
    "bow = TfidfVectorizer(min_df=0.01, max_df=0.95)  #\n",
    "bow.fit(corpus)\n",
    "print(len(bow.vocabulary_))\n",
    "\n",
    "def get_features(d, ind, data_type=\"train\"):\n",
    "    features = []\n",
    "\n",
    "    # mean # of likes\n",
    "    user =  d['userid']\n",
    "    if user in account_likes:\n",
    "        features.append(0)\n",
    "        features.append(account_likes[user])\n",
    "    else:\n",
    "        features.append(1)\n",
    "        features.append(0)\n",
    "\n",
    "    # Twitter client\n",
    "    client_features = [0 for i in range(len(clients))]\n",
    "    client = d['tweet_client_name']\n",
    "    if client in clients:\n",
    "        features.append(0)\n",
    "        client_index = clients[client]\n",
    "        client_features[client_index] = 1\n",
    "    else:\n",
    "        features.append(1)\n",
    "    features.extend(client_features)\n",
    "\n",
    "    # popular hashtags\n",
    "    curr_hashtags = d['hashtags'][1:-1].split(', ')\n",
    "    hashtag_features = [0 for i in range(len(popular_hashtags))]\n",
    "    for ht in curr_hashtags:\n",
    "        if ht in popular_hashtags:\n",
    "            ht_index = popular_hashtags[ht]\n",
    "            hashtag_features[ht_index] = 1\n",
    "    features.extend(hashtag_features)\n",
    "\n",
    "    # url\n",
    "    if 'https://t.co' in d['tweet_text'] or d['urls'] != '':\n",
    "        features.append(1)\n",
    "    else:\n",
    "        features.append(0)\n",
    "\n",
    "    # follower count\n",
    "    features.append(int(d['follower_count']))\n",
    "\n",
    "    # BOW\n",
    "    features.extend(bow.transform([d['tweet_text']]).toarray()[0])\n",
    "\n",
    "    # Time since epoch\n",
    "    date = datetime.strptime(d['tweet_time'], '%Y-%m-%d %H:%M')\n",
    "    features.append(date.timestamp())\n",
    "\n",
    "    # Time of day\n",
    "    minutes_in_day = 24 * 60\n",
    "    minutes = date.hour * 60 + date.minute\n",
    "    sin_time = np.sin(2 * np.pi * minutes / minutes_in_day)\n",
    "    cos_time = np.cos(2 * np.pi * minutes / minutes_in_day)\n",
    "\n",
    "    features.append(sin_time)\n",
    "    features.append(cos_time)\n",
    "    \n",
    "    # cluster features\n",
    "    if data_type == \"train\" or data_type == \"valid\":\n",
    "        features.extend(ohe_tweet_train_cluster_labels[ind].tolist())\n",
    "    else:\n",
    "        features.extend(ohe_tweet_test_cluster_labels[ind].tolist())\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "for ind,d in enumerate(train):\n",
    "    X_train.append(get_features(d, ind))\n",
    "    if d['like_count'] == '0':\n",
    "        y_train.append(0)\n",
    "    else:\n",
    "        y_train.append(1)\n",
    "\n",
    "X_valid = []\n",
    "y_valid = []\n",
    "for ind, d in enumerate(valid):\n",
    "    X_valid.append(get_features(d, split+ind))\n",
    "    like_count = int(d['like_count'])\n",
    "    if like_count == 0:\n",
    "        y_valid.append(0)\n",
    "    else:\n",
    "        y_valid.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "X_valid = np.array(X_valid, dtype=np.float32)\n",
    "y_valid = np.array(y_valid, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 187)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85044\n",
      "0.84604\n"
     ]
    }
   ],
   "source": [
    "# best_score = 0\n",
    "# best_lr = None\n",
    "# possible_lrs = [0.1, 0.3, 0.5, 0.7, 1, 1.4, 1.6, 2.3, 4]\n",
    "# for lr in possible_lrs:\n",
    "ada = AdaBoostClassifier(learning_rate=1.6)\n",
    "ada.fit(list(X_train), y_train)\n",
    "print(ada.score(X_train, y_train))\n",
    "valid_Score = ada.score(X_valid, y_valid)\n",
    "    \n",
    "#     if valid_Score > best_score:\n",
    "#         best_score = valid_Score\n",
    "#         best_lr = lr\n",
    "print(valid_Score)\n",
    "# print(best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# follower count, url, client, cluster labels\n",
    "# 0.83812\n",
    "# 0.8356\n",
    "\n",
    "# median likes, follower count, url, client, cluster labels\n",
    "# 0.846\n",
    "# 0.84128\n",
    "\n",
    "# median likes, follower count, url, client, time since epoch, time of day, cluster labels\n",
    "# 0.8496\n",
    "# 0.84384\n",
    "\n",
    "# median likes, top 10 hashtags, follower count, url, client, time since epoch, time of day, cluster labels\n",
    "# 0.84972\n",
    "# 0.84376\n",
    "\n",
    "# BOW with min/max df, median likes, top 10 hashtags, follower count, url, client, \n",
    "# time since epoch, time of day, cluster labels\n",
    "# 0.85044\n",
    "# 0.84604"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
